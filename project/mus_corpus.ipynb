{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "import spotipy\n",
    "import lyricsgenius as genius\n",
    "import requests\n",
    "import warnings\n",
    "import shutil\n",
    "import conf\n",
    "\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from spotipy import util\n",
    "from lyricsgenius.utils import sanitize_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cid = conf.S_TOKEN\n",
    "secret = conf.S_SECRET\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id=cid, client_secret=secret) \n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('25_Hello.txt', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "text = re.sub('\\[.*\\]', '', text)\n",
    "text = text.splitlines()\n",
    "while '' in text:\n",
    "    text.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df, \n",
    "columns=['line','lemma', 'part of speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "      <th>lemma</th>\n",
       "      <th>part of speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello, it's me</td>\n",
       "      <td>[hello, it, be, me]</td>\n",
       "      <td>[INTJ, PRON, AUX, PRON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was wondering if after all these years you'd...</td>\n",
       "      <td>[I, be, wonder, if, after, all, these, year, y...</td>\n",
       "      <td>[PRON, AUX, VERB, SCONJ, ADP, DET, DET, NOUN, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To go over everything</td>\n",
       "      <td>[to, go, over, everything]</td>\n",
       "      <td>[PART, VERB, ADP, PRON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They say that time's supposed to heal ya, but ...</td>\n",
       "      <td>[They, say, that, time, suppose, to, heal, ya,...</td>\n",
       "      <td>[PRON, VERB, DET, NOUN, VERB, PART, VERB, PRON...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hello, can you hear me?</td>\n",
       "      <td>[hello, can, you, hear, me]</td>\n",
       "      <td>[INTJ, VERB, PRON, VERB, PRON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>But when I call, you never seem to be home</td>\n",
       "      <td>[but, when, I, call, you, never, seem, to, be,...</td>\n",
       "      <td>[CCONJ, ADV, PRON, VERB, PRON, ADV, VERB, PART...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Hello from the outside</td>\n",
       "      <td>[hello, from, the, outside]</td>\n",
       "      <td>[INTJ, ADP, DET, NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>At least, I can say that I've tried</td>\n",
       "      <td>[at, least, I, can, say, that, I, have, try]</td>\n",
       "      <td>[ADV, ADV, PRON, VERB, VERB, SCONJ, PRON, AUX,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>To tell you I'm sorry for breaking your heart</td>\n",
       "      <td>[to, tell, you, I, be, sorry, for, break, your...</td>\n",
       "      <td>[PART, VERB, PRON, PRON, AUX, ADJ, ADP, VERB, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>But it don't matter, it clearly doesn't tear y...</td>\n",
       "      <td>[but, it, do, not, matter, it, clearly, do, no...</td>\n",
       "      <td>[CCONJ, PRON, AUX, PART, VERB, PRON, ADV, AUX,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>480 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 line  \\\n",
       "0                                      Hello, it's me   \n",
       "1   I was wondering if after all these years you'd...   \n",
       "2                               To go over everything   \n",
       "3   They say that time's supposed to heal ya, but ...   \n",
       "4                             Hello, can you hear me?   \n",
       "..                                                ...   \n",
       "43         But when I call, you never seem to be home   \n",
       "44                             Hello from the outside   \n",
       "45                At least, I can say that I've tried   \n",
       "46      To tell you I'm sorry for breaking your heart   \n",
       "47  But it don't matter, it clearly doesn't tear y...   \n",
       "\n",
       "                                                lemma  \\\n",
       "0                                 [hello, it, be, me]   \n",
       "1   [I, be, wonder, if, after, all, these, year, y...   \n",
       "2                          [to, go, over, everything]   \n",
       "3   [They, say, that, time, suppose, to, heal, ya,...   \n",
       "4                         [hello, can, you, hear, me]   \n",
       "..                                                ...   \n",
       "43  [but, when, I, call, you, never, seem, to, be,...   \n",
       "44                        [hello, from, the, outside]   \n",
       "45       [at, least, I, can, say, that, I, have, try]   \n",
       "46  [to, tell, you, I, be, sorry, for, break, your...   \n",
       "47  [but, it, do, not, matter, it, clearly, do, no...   \n",
       "\n",
       "                                       part of speech  \n",
       "0                             [INTJ, PRON, AUX, PRON]  \n",
       "1   [PRON, AUX, VERB, SCONJ, ADP, DET, DET, NOUN, ...  \n",
       "2                             [PART, VERB, ADP, PRON]  \n",
       "3   [PRON, VERB, DET, NOUN, VERB, PART, VERB, PRON...  \n",
       "4                      [INTJ, VERB, PRON, VERB, PRON]  \n",
       "..                                                ...  \n",
       "43  [CCONJ, ADV, PRON, VERB, PRON, ADV, VERB, PART...  \n",
       "44                             [INTJ, ADP, DET, NOUN]  \n",
       "45  [ADV, ADV, PRON, VERB, VERB, SCONJ, PRON, AUX,...  \n",
       "46  [PART, VERB, PRON, PRON, AUX, ADJ, ADP, VERB, ...  \n",
       "47  [CCONJ, PRON, AUX, PART, VERB, PRON, ADV, AUX,...  \n",
       "\n",
       "[480 rows x 3 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, it's me\n",
      "[\"Hello, it's me\", ['hello', 'it', 'be', 'me'], ['INTJ', 'PRON', 'AUX', 'PRON']]\n",
      "I was wondering if after all these years you'd like to meet\n",
      "[\"I was wondering if after all these years you'd like to meet\", ['I', 'be', 'wonder', 'if', 'after', 'all', 'these', 'year', 'you', 'like', 'to', 'meet'], ['PRON', 'AUX', 'VERB', 'SCONJ', 'ADP', 'DET', 'DET', 'NOUN', 'PRON', 'VERB', 'PART', 'VERB']]\n",
      "To go over everything\n",
      "['To go over everything', ['to', 'go', 'over', 'everything'], ['PART', 'VERB', 'ADP', 'PRON']]\n",
      "They say that time's supposed to heal ya, but I ain't done much healing\n",
      "[\"They say that time's supposed to heal ya, but I ain't done much healing\", ['They', 'say', 'that', 'time', 'suppose', 'to', 'heal', 'ya', 'but', 'I', 'be', 'not', 'do', 'much', 'healing'], ['PRON', 'VERB', 'DET', 'NOUN', 'VERB', 'PART', 'VERB', 'PRON', 'CCONJ', 'PRON', 'VERB', 'PART', 'VERB', 'ADJ', 'NOUN']]\n",
      "Hello, can you hear me?\n",
      "['Hello, can you hear me?', ['hello', 'can', 'you', 'hear', 'me'], ['INTJ', 'VERB', 'PRON', 'VERB', 'PRON']]\n",
      "I'm in California dreaming about who we used to be\n",
      "[\"I'm in California dreaming about who we used to be\", ['I', 'be', 'in', 'California', 'dream', 'about', 'who', 'we', 'use', 'to', 'be'], ['PRON', 'AUX', 'ADP', 'PROPN', 'VERB', 'ADP', 'PRON', 'PRON', 'VERB', 'PART', 'AUX']]\n",
      "When we were younger and free\n",
      "['When we were younger and free', ['when', 'we', 'be', 'young', 'and', 'free'], ['ADV', 'PRON', 'AUX', 'ADJ', 'CCONJ', 'ADJ']]\n",
      "I've forgotten how it felt before the world fell at our feet\n",
      "[\"I've forgotten how it felt before the world fell at our feet\", ['I', 'have', 'forget', 'how', 'it', 'feel', 'before', 'the', 'world', 'fall', 'at', 'our', 'foot'], ['PRON', 'AUX', 'VERB', 'ADV', 'PRON', 'VERB', 'ADP', 'DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN']]\n",
      "There's such a difference between us\n",
      "[\"There's such a difference between us\", ['there', 'be', 'such', 'a', 'difference', 'between', 'us'], ['PRON', 'AUX', 'DET', 'DET', 'NOUN', 'ADP', 'PRON']]\n",
      "And a million miles\n",
      "['And a million miles', ['and', 'a', 'million', 'mile'], ['CCONJ', 'DET', 'NUM', 'NOUN']]\n",
      "Hello from the other side\n",
      "['Hello from the other side', ['hello', 'from', 'the', 'other', 'side'], ['INTJ', 'ADP', 'DET', 'ADJ', 'NOUN']]\n",
      "I must've called a thousand times\n",
      "[\"I must've called a thousand times\", ['I', 'must', 'have', 'call', 'a', 'thousand', 'time'], ['PRON', 'VERB', 'AUX', 'VERB', 'DET', 'NUM', 'NOUN']]\n",
      "To tell you I'm sorry for everything that I've done\n",
      "[\"To tell you I'm sorry for everything that I've done\", ['to', 'tell', 'you', 'I', 'be', 'sorry', 'for', 'everything', 'that', 'I', 'have', 'do'], ['PART', 'VERB', 'PRON', 'PRON', 'AUX', 'ADJ', 'ADP', 'PRON', 'DET', 'PRON', 'AUX', 'VERB']]\n",
      "But when I call, you never seem to be home\n",
      "['But when I call, you never seem to be home', ['but', 'when', 'I', 'call', 'you', 'never', 'seem', 'to', 'be', 'home'], ['CCONJ', 'ADV', 'PRON', 'VERB', 'PRON', 'ADV', 'VERB', 'PART', 'AUX', 'ADV']]\n",
      "Hello from the outside\n",
      "['Hello from the outside', ['hello', 'from', 'the', 'outside'], ['INTJ', 'ADP', 'DET', 'NOUN']]\n",
      "At least, I can say that I've tried\n",
      "[\"At least, I can say that I've tried\", ['at', 'least', 'I', 'can', 'say', 'that', 'I', 'have', 'try'], ['ADV', 'ADV', 'PRON', 'VERB', 'VERB', 'SCONJ', 'PRON', 'AUX', 'VERB']]\n",
      "To tell you I'm sorry for breaking your heart\n",
      "[\"To tell you I'm sorry for breaking your heart\", ['to', 'tell', 'you', 'I', 'be', 'sorry', 'for', 'break', 'your', 'heart'], ['PART', 'VERB', 'PRON', 'PRON', 'AUX', 'ADJ', 'ADP', 'VERB', 'DET', 'NOUN']]\n",
      "But it don't matter, it clearly doesn't tear you apart anymore\n",
      "[\"But it don't matter, it clearly doesn't tear you apart anymore\", ['but', 'it', 'do', 'not', 'matter', 'it', 'clearly', 'do', 'not', 'tear', 'you', 'apart', 'anymore'], ['CCONJ', 'PRON', 'AUX', 'PART', 'VERB', 'PRON', 'ADV', 'AUX', 'PART', 'VERB', 'PRON', 'ADV', 'ADV']]\n",
      "Hello, how are you?\n",
      "['Hello, how are you?', ['hello', 'how', 'be', 'you'], ['INTJ', 'ADV', 'AUX', 'PRON']]\n",
      "It's so typical of me to talk about myself, I'm sorry\n",
      "[\"It's so typical of me to talk about myself, I'm sorry\", ['It', 'be', 'so', 'typical', 'of', 'me', 'to', 'talk', 'about', 'myself', 'I', 'be', 'sorry'], ['PRON', 'AUX', 'ADV', 'ADJ', 'ADP', 'PRON', 'PART', 'VERB', 'ADP', 'PRON', 'PRON', 'AUX', 'ADJ']]\n",
      "I hope that you're well\n",
      "[\"I hope that you're well\", ['I', 'hope', 'that', 'you', 'be', 'well'], ['PRON', 'VERB', 'SCONJ', 'PRON', 'AUX', 'ADV']]\n",
      "Did you ever make it out of that town where nothing ever happened?\n",
      "['Did you ever make it out of that town where nothing ever happened?', ['do', 'you', 'ever', 'make', 'it', 'out', 'of', 'that', 'town', 'where', 'nothing', 'ever', 'happen'], ['AUX', 'PRON', 'ADV', 'VERB', 'PRON', 'SCONJ', 'ADP', 'DET', 'NOUN', 'ADV', 'PRON', 'ADV', 'VERB']]\n",
      "It's no secret that the both of us\n",
      "[\"It's no secret that the both of us\", ['It', 'be', 'no', 'secret', 'that', 'the', 'both', 'of', 'us'], ['PRON', 'AUX', 'DET', 'NOUN', 'SCONJ', 'DET', 'DET', 'ADP', 'PRON']]\n",
      "Are running out of time\n",
      "['Are running out of time', ['be', 'run', 'out', 'of', 'time'], ['AUX', 'VERB', 'SCONJ', 'ADP', 'NOUN']]\n",
      "So hello from the other side\n",
      "['So hello from the other side', ['so', 'hello', 'from', 'the', 'other', 'side'], ['ADV', 'INTJ', 'ADP', 'DET', 'ADJ', 'NOUN']]\n",
      "I must've called a thousand times\n",
      "[\"I must've called a thousand times\", ['I', 'must', 'have', 'call', 'a', 'thousand', 'time'], ['PRON', 'VERB', 'AUX', 'VERB', 'DET', 'NUM', 'NOUN']]\n",
      "To tell you I'm sorry for everything that I've done\n",
      "[\"To tell you I'm sorry for everything that I've done\", ['to', 'tell', 'you', 'I', 'be', 'sorry', 'for', 'everything', 'that', 'I', 'have', 'do'], ['PART', 'VERB', 'PRON', 'PRON', 'AUX', 'ADJ', 'ADP', 'PRON', 'DET', 'PRON', 'AUX', 'VERB']]\n",
      "But when I call, you never seem to be home\n",
      "['But when I call, you never seem to be home', ['but', 'when', 'I', 'call', 'you', 'never', 'seem', 'to', 'be', 'home'], ['CCONJ', 'ADV', 'PRON', 'VERB', 'PRON', 'ADV', 'VERB', 'PART', 'AUX', 'ADV']]\n",
      "Hello from the outside\n",
      "['Hello from the outside', ['hello', 'from', 'the', 'outside'], ['INTJ', 'ADP', 'DET', 'NOUN']]\n",
      "At least, I can say that I've tried\n",
      "[\"At least, I can say that I've tried\", ['at', 'least', 'I', 'can', 'say', 'that', 'I', 'have', 'try'], ['ADV', 'ADV', 'PRON', 'VERB', 'VERB', 'SCONJ', 'PRON', 'AUX', 'VERB']]\n",
      "To tell you I'm sorry for breaking your heart\n",
      "[\"To tell you I'm sorry for breaking your heart\", ['to', 'tell', 'you', 'I', 'be', 'sorry', 'for', 'break', 'your', 'heart'], ['PART', 'VERB', 'PRON', 'PRON', 'AUX', 'ADJ', 'ADP', 'VERB', 'DET', 'NOUN']]\n",
      "But it don't matter, it clearly doesn't tear you apart anymore\n",
      "[\"But it don't matter, it clearly doesn't tear you apart anymore\", ['but', 'it', 'do', 'not', 'matter', 'it', 'clearly', 'do', 'not', 'tear', 'you', 'apart', 'anymore'], ['CCONJ', 'PRON', 'AUX', 'PART', 'VERB', 'PRON', 'ADV', 'AUX', 'PART', 'VERB', 'PRON', 'ADV', 'ADV']]\n",
      "(Highs, highs, highs, highs, lows, lows, lows, lows)\n",
      "['(Highs, highs, highs, highs, lows, lows, lows, lows)', ['high', 'high', 'high', 'high', 'low', 'low', 'low', 'low'], ['NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN']]\n",
      "Ooh, anymore\n",
      "['Ooh, anymore', ['ooh', 'anymore'], ['ADV', 'ADV']]\n",
      "(Highs, highs, highs, highs, lows, lows, lows, lows)\n",
      "['(Highs, highs, highs, highs, lows, lows, lows, lows)', ['high', 'high', 'high', 'high', 'low', 'low', 'low', 'low'], ['NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN']]\n",
      "Ooh, anymore\n",
      "['Ooh, anymore', ['ooh', 'anymore'], ['ADV', 'ADV']]\n",
      "(Highs, highs, highs, highs, lows, lows, lows, lows)\n",
      "['(Highs, highs, highs, highs, lows, lows, lows, lows)', ['high', 'high', 'high', 'high', 'low', 'low', 'low', 'low'], ['NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN']]\n",
      "Ooh, anymore\n",
      "['Ooh, anymore', ['ooh', 'anymore'], ['ADV', 'ADV']]\n",
      "(Highs, highs, highs, highs, lows, lows, lows, lows)\n",
      "['(Highs, highs, highs, highs, lows, lows, lows, lows)', ['high', 'high', 'high', 'high', 'low', 'low', 'low', 'low'], ['NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN']]\n",
      "Anymore\n",
      "['Anymore', ['anymore'], ['ADV']]\n",
      "Hello from the other side\n",
      "['Hello from the other side', ['hello', 'from', 'the', 'other', 'side'], ['INTJ', 'ADP', 'DET', 'ADJ', 'NOUN']]\n",
      "I must've called a thousand times\n",
      "[\"I must've called a thousand times\", ['I', 'must', 'have', 'call', 'a', 'thousand', 'time'], ['PRON', 'VERB', 'AUX', 'VERB', 'DET', 'NUM', 'NOUN']]\n",
      "To tell you I'm sorry for everything that I've done\n",
      "[\"To tell you I'm sorry for everything that I've done\", ['to', 'tell', 'you', 'I', 'be', 'sorry', 'for', 'everything', 'that', 'I', 'have', 'do'], ['PART', 'VERB', 'PRON', 'PRON', 'AUX', 'ADJ', 'ADP', 'PRON', 'DET', 'PRON', 'AUX', 'VERB']]\n",
      "But when I call, you never seem to be home\n",
      "['But when I call, you never seem to be home', ['but', 'when', 'I', 'call', 'you', 'never', 'seem', 'to', 'be', 'home'], ['CCONJ', 'ADV', 'PRON', 'VERB', 'PRON', 'ADV', 'VERB', 'PART', 'AUX', 'ADV']]\n",
      "Hello from the outside\n",
      "['Hello from the outside', ['hello', 'from', 'the', 'outside'], ['INTJ', 'ADP', 'DET', 'NOUN']]\n",
      "At least, I can say that I've tried\n",
      "[\"At least, I can say that I've tried\", ['at', 'least', 'I', 'can', 'say', 'that', 'I', 'have', 'try'], ['ADV', 'ADV', 'PRON', 'VERB', 'VERB', 'SCONJ', 'PRON', 'AUX', 'VERB']]\n",
      "To tell you I'm sorry for breaking your heart\n",
      "[\"To tell you I'm sorry for breaking your heart\", ['to', 'tell', 'you', 'I', 'be', 'sorry', 'for', 'break', 'your', 'heart'], ['PART', 'VERB', 'PRON', 'PRON', 'AUX', 'ADJ', 'ADP', 'VERB', 'DET', 'NOUN']]\n",
      "But it don't matter, it clearly doesn't tear you apart anymore\n",
      "[\"But it don't matter, it clearly doesn't tear you apart anymore\", ['but', 'it', 'do', 'not', 'matter', 'it', 'clearly', 'do', 'not', 'tear', 'you', 'apart', 'anymore'], ['CCONJ', 'PRON', 'AUX', 'PART', 'VERB', 'PRON', 'ADV', 'AUX', 'PART', 'VERB', 'PRON', 'ADV', 'ADV']]\n"
     ]
    }
   ],
   "source": [
    "df = []\n",
    "\n",
    "for li in text:\n",
    "    l = []\n",
    "    line = []\n",
    "    pos = []\n",
    "    doc = nlp(li)\n",
    "    for token in doc:\n",
    "        if token.lemma_.isalpha():\n",
    "            l.append(token.lemma_)\n",
    "            pos.append(token.pos_)\n",
    "        elif token.lemma_ == '-PRON-':\n",
    "            l.append(str(token))\n",
    "            pos.append(token.pos_)\n",
    "    line.append(li)\n",
    "    line.append(l)\n",
    "    line.append(pos)\n",
    "    print(li)\n",
    "    print(line)\n",
    "    df.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for line in text:\n",
    "    \n",
    "    doc = nlp(line)\n",
    "    for token in doc:\n",
    "        l = [line]\n",
    "        if token.lemma_.isalpha():\n",
    "            l.append(token.lemma_)\n",
    "            l.append(token.pos_)\n",
    "            df.append(l)\n",
    "        elif token.lemma_ == '-PRON-':\n",
    "            l.append(str(token))\n",
    "            l.append(token.pos_)\n",
    "            df.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df, \n",
    "columns=['line','lemma', 'part of speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "      <th>lemma</th>\n",
       "      <th>part of speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello, it's me</td>\n",
       "      <td>hello</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello, it's me</td>\n",
       "      <td>it</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hello, it's me</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hello, it's me</td>\n",
       "      <td>me</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was wondering if after all these years you'd...</td>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I was wondering if after all these years you'd...</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I was wondering if after all these years you'd...</td>\n",
       "      <td>wonder</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I was wondering if after all these years you'd...</td>\n",
       "      <td>if</td>\n",
       "      <td>SCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I was wondering if after all these years you'd...</td>\n",
       "      <td>after</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I was wondering if after all these years you'd...</td>\n",
       "      <td>all</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                line   lemma part of speech\n",
       "0                                     Hello, it's me   hello           INTJ\n",
       "1                                     Hello, it's me      it           PRON\n",
       "2                                     Hello, it's me      be            AUX\n",
       "3                                     Hello, it's me      me           PRON\n",
       "4  I was wondering if after all these years you'd...       I           PRON\n",
       "5  I was wondering if after all these years you'd...      be            AUX\n",
       "6  I was wondering if after all these years you'd...  wonder           VERB\n",
       "7  I was wondering if after all these years you'd...      if          SCONJ\n",
       "8  I was wondering if after all these years you'd...   after            ADP\n",
       "9  I was wondering if after all these years you'd...     all            DET"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['lemma'] == n[0]]\n",
    "df.index[df['lemma'] == n[1]][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "      <th>lemma</th>\n",
       "      <th>part of speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello, it's me</td>\n",
       "      <td>hello</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello, it's me</td>\n",
       "      <td>it</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hello, it's me</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hello, it's me</td>\n",
       "      <td>me</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was wondering if after all these years you'd...</td>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>But it don't matter, it clearly doesn't tear y...</td>\n",
       "      <td>not</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>But it don't matter, it clearly doesn't tear y...</td>\n",
       "      <td>tear</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>But it don't matter, it clearly doesn't tear y...</td>\n",
       "      <td>you</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>But it don't matter, it clearly doesn't tear y...</td>\n",
       "      <td>apart</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>But it don't matter, it clearly doesn't tear y...</td>\n",
       "      <td>anymore</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>381 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  line    lemma part of speech\n",
       "0                                       Hello, it's me    hello           INTJ\n",
       "1                                       Hello, it's me       it           PRON\n",
       "2                                       Hello, it's me       be            AUX\n",
       "3                                       Hello, it's me       me           PRON\n",
       "4    I was wondering if after all these years you'd...        I           PRON\n",
       "..                                                 ...      ...            ...\n",
       "376  But it don't matter, it clearly doesn't tear y...      not           PART\n",
       "377  But it don't matter, it clearly doesn't tear y...     tear           VERB\n",
       "378  But it don't matter, it clearly doesn't tear y...      you           PRON\n",
       "379  But it don't matter, it clearly doesn't tear y...    apart            ADV\n",
       "380  But it don't matter, it clearly doesn't tear y...  anymore            ADV\n",
       "\n",
       "[381 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'it',\n",
       " 'be',\n",
       " 'me',\n",
       " 'I',\n",
       " 'be',\n",
       " 'wonder',\n",
       " 'if',\n",
       " 'after',\n",
       " 'all',\n",
       " 'these',\n",
       " 'year',\n",
       " 'you',\n",
       " 'like',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'to',\n",
       " 'go',\n",
       " 'over',\n",
       " 'everything',\n",
       " 'They',\n",
       " 'say',\n",
       " 'that',\n",
       " 'time',\n",
       " 'suppose',\n",
       " 'to',\n",
       " 'heal',\n",
       " 'ya',\n",
       " 'but',\n",
       " 'I',\n",
       " 'be',\n",
       " 'not',\n",
       " 'do',\n",
       " 'much',\n",
       " 'healing',\n",
       " 'hello',\n",
       " 'can',\n",
       " 'you',\n",
       " 'hear',\n",
       " 'me',\n",
       " 'I',\n",
       " 'be',\n",
       " 'in',\n",
       " 'California',\n",
       " 'dream',\n",
       " 'about',\n",
       " 'who',\n",
       " 'we',\n",
       " 'use',\n",
       " 'to',\n",
       " 'be',\n",
       " 'when',\n",
       " 'we',\n",
       " 'be',\n",
       " 'young',\n",
       " 'and',\n",
       " 'free',\n",
       " 'I',\n",
       " 'have',\n",
       " 'forget',\n",
       " 'how',\n",
       " 'it',\n",
       " 'feel',\n",
       " 'before',\n",
       " 'the',\n",
       " 'world',\n",
       " 'fall',\n",
       " 'at',\n",
       " 'our',\n",
       " 'foot',\n",
       " 'there',\n",
       " 'be',\n",
       " 'such',\n",
       " 'a',\n",
       " 'difference',\n",
       " 'between',\n",
       " 'us',\n",
       " 'and',\n",
       " 'a',\n",
       " 'million',\n",
       " 'mile',\n",
       " 'hello',\n",
       " 'from',\n",
       " 'the',\n",
       " 'other',\n",
       " 'side',\n",
       " 'I',\n",
       " 'must',\n",
       " 'have',\n",
       " 'call',\n",
       " 'a',\n",
       " 'thousand',\n",
       " 'time',\n",
       " 'to',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'I',\n",
       " 'be',\n",
       " 'sorry',\n",
       " 'for',\n",
       " 'everything',\n",
       " 'that',\n",
       " 'I',\n",
       " 'have',\n",
       " 'do',\n",
       " 'but',\n",
       " 'when',\n",
       " 'I',\n",
       " 'call',\n",
       " 'you',\n",
       " 'never',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'be',\n",
       " 'home',\n",
       " 'hello',\n",
       " 'from',\n",
       " 'the',\n",
       " 'outside',\n",
       " 'at',\n",
       " 'least',\n",
       " 'I',\n",
       " 'can',\n",
       " 'say',\n",
       " 'that',\n",
       " 'I',\n",
       " 'have',\n",
       " 'try',\n",
       " 'to',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'I',\n",
       " 'be',\n",
       " 'sorry',\n",
       " 'for',\n",
       " 'break',\n",
       " 'your',\n",
       " 'heart',\n",
       " 'but',\n",
       " 'it',\n",
       " 'do',\n",
       " 'not',\n",
       " 'matter',\n",
       " 'it',\n",
       " 'clearly',\n",
       " 'do',\n",
       " 'not',\n",
       " 'tear',\n",
       " 'you',\n",
       " 'apart',\n",
       " 'anymore',\n",
       " 'hello',\n",
       " 'how',\n",
       " 'be',\n",
       " 'you',\n",
       " 'It',\n",
       " 'be',\n",
       " 'so',\n",
       " 'typical',\n",
       " 'of',\n",
       " 'me',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'about',\n",
       " 'myself',\n",
       " 'I',\n",
       " 'be',\n",
       " 'sorry',\n",
       " 'I',\n",
       " 'hope',\n",
       " 'that',\n",
       " 'you',\n",
       " 'be',\n",
       " 'well',\n",
       " 'do',\n",
       " 'you',\n",
       " 'ever',\n",
       " 'make',\n",
       " 'it',\n",
       " 'out',\n",
       " 'of',\n",
       " 'that',\n",
       " 'town',\n",
       " 'where',\n",
       " 'nothing',\n",
       " 'ever',\n",
       " 'happen',\n",
       " 'It',\n",
       " 'be',\n",
       " 'no',\n",
       " 'secret',\n",
       " 'that',\n",
       " 'the',\n",
       " 'both',\n",
       " 'of',\n",
       " 'us',\n",
       " 'be',\n",
       " 'run',\n",
       " 'out',\n",
       " 'of',\n",
       " 'time',\n",
       " 'so',\n",
       " 'hello',\n",
       " 'from',\n",
       " 'the',\n",
       " 'other',\n",
       " 'side',\n",
       " 'I',\n",
       " 'must',\n",
       " 'have',\n",
       " 'call',\n",
       " 'a',\n",
       " 'thousand',\n",
       " 'time',\n",
       " 'to',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'I',\n",
       " 'be',\n",
       " 'sorry',\n",
       " 'for',\n",
       " 'everything',\n",
       " 'that',\n",
       " 'I',\n",
       " 'have',\n",
       " 'do',\n",
       " 'but',\n",
       " 'when',\n",
       " 'I',\n",
       " 'call',\n",
       " 'you',\n",
       " 'never',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'be',\n",
       " 'home',\n",
       " 'hello',\n",
       " 'from',\n",
       " 'the',\n",
       " 'outside',\n",
       " 'at',\n",
       " 'least',\n",
       " 'I',\n",
       " 'can',\n",
       " 'say',\n",
       " 'that',\n",
       " 'I',\n",
       " 'have',\n",
       " 'try',\n",
       " 'to',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'I',\n",
       " 'be',\n",
       " 'sorry',\n",
       " 'for',\n",
       " 'break',\n",
       " 'your',\n",
       " 'heart',\n",
       " 'but',\n",
       " 'it',\n",
       " 'do',\n",
       " 'not',\n",
       " 'matter',\n",
       " 'it',\n",
       " 'clearly',\n",
       " 'do',\n",
       " 'not',\n",
       " 'tear',\n",
       " 'you',\n",
       " 'apart',\n",
       " 'anymore',\n",
       " 'high',\n",
       " 'high',\n",
       " 'high',\n",
       " 'high',\n",
       " 'low',\n",
       " 'low',\n",
       " 'low',\n",
       " 'low',\n",
       " 'ooh',\n",
       " 'anymore',\n",
       " 'high',\n",
       " 'high',\n",
       " 'high',\n",
       " 'high',\n",
       " 'low',\n",
       " 'low',\n",
       " 'low',\n",
       " 'low',\n",
       " 'ooh',\n",
       " 'anymore',\n",
       " 'high',\n",
       " 'high',\n",
       " 'high',\n",
       " 'high',\n",
       " 'low',\n",
       " 'low',\n",
       " 'low',\n",
       " 'low',\n",
       " 'ooh',\n",
       " 'anymore',\n",
       " 'high',\n",
       " 'high',\n",
       " 'high',\n",
       " 'high',\n",
       " 'low',\n",
       " 'low',\n",
       " 'low',\n",
       " 'low',\n",
       " 'anymore',\n",
       " 'hello',\n",
       " 'from',\n",
       " 'the',\n",
       " 'other',\n",
       " 'side',\n",
       " 'I',\n",
       " 'must',\n",
       " 'have',\n",
       " 'call',\n",
       " 'a',\n",
       " 'thousand',\n",
       " 'time',\n",
       " 'to',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'I',\n",
       " 'be',\n",
       " 'sorry',\n",
       " 'for',\n",
       " 'everything',\n",
       " 'that',\n",
       " 'I',\n",
       " 'have',\n",
       " 'do',\n",
       " 'but',\n",
       " 'when',\n",
       " 'I',\n",
       " 'call',\n",
       " 'you',\n",
       " 'never',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'be',\n",
       " 'home',\n",
       " 'hello',\n",
       " 'from',\n",
       " 'the',\n",
       " 'outside',\n",
       " 'at',\n",
       " 'least',\n",
       " 'I',\n",
       " 'can',\n",
       " 'say',\n",
       " 'that',\n",
       " 'I',\n",
       " 'have',\n",
       " 'try',\n",
       " 'to',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'I',\n",
       " 'be',\n",
       " 'sorry',\n",
       " 'for',\n",
       " 'break',\n",
       " 'your',\n",
       " 'heart',\n",
       " 'but',\n",
       " 'it',\n",
       " 'do',\n",
       " 'not',\n",
       " 'matter',\n",
       " 'it',\n",
       " 'clearly',\n",
       " 'do',\n",
       " 'not',\n",
       " 'tear',\n",
       " 'you',\n",
       " 'apart',\n",
       " 'anymore']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be+me\n",
      "             line lemma part of speech\n",
      "2  Hello, it's me    be            AUX\n"
     ]
    }
   ],
   "source": [
    "a = input()\n",
    "n = a.split('+')\n",
    "lemmas = df.lemma.tolist()\n",
    "\n",
    "for i in range(len(lemmas)):\n",
    "    j = len(n) + i\n",
    "    if lemmas[i:j] == n:\n",
    "        print(df[df.index == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "      <th>lemma</th>\n",
       "      <th>part of speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello, it's me</td>\n",
       "      <td>hello</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             line  lemma part of speech\n",
       "0  Hello, it's me  hello           INTJ"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.index == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "tex = 'Who killed them?'\n",
    "d = nlp(tex)\n",
    "u = d[2].i\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pron_lemma = {\"they\" : [\"they\", \"their\", \"them\", \"theirs\"], \n",
    "              \"I\" : [\"me\", \"myself\", \"I\", \"my\"], \n",
    "              \"he\" : [\"he\", \"his\", \"him\", \"himself\"], \n",
    "              \"she\" : [\"she\", \"her\", \"herself\"], \n",
    "              \"it\" : [\"its\", \"it\", \"itself\"], \n",
    "              \"you\" : [\"you\", \"your\", \"yours\", \"u\"]}\n",
    "df = []\n",
    "for line in text:\n",
    "    doc = nlp(line)\n",
    "    for token in doc:\n",
    "        l = [line]\n",
    "        if token.lemma_.isalpha():\n",
    "            l.append(token.lemma_)\n",
    "            l.append(token.pos_)\n",
    "            df.append(l)\n",
    "        elif token.lemma_ == '-PRON-':\n",
    "            for pronoun in pron_lemma.items():\n",
    "                if token.text in pronoun[1]:\n",
    "                    l.append(pronoun[0])\n",
    "            l.append(token.pos_)\n",
    "            df.append(l)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"Hello, it's me\", 'hello', 'INTJ'],\n",
       " [\"Hello, it's me\", 'it', 'PRON'],\n",
       " [\"Hello, it's me\", 'be', 'AUX'],\n",
       " [\"Hello, it's me\", 'I', 'PRON'],\n",
       " [\"I was wondering if after all these years you'd like to meet\", 'I', 'PRON'],\n",
       " [\"I was wondering if after all these years you'd like to meet\", 'be', 'AUX'],\n",
       " [\"I was wondering if after all these years you'd like to meet\",\n",
       "  'wonder',\n",
       "  'VERB'],\n",
       " [\"I was wondering if after all these years you'd like to meet\",\n",
       "  'if',\n",
       "  'SCONJ'],\n",
       " [\"I was wondering if after all these years you'd like to meet\",\n",
       "  'after',\n",
       "  'ADP'],\n",
       " [\"I was wondering if after all these years you'd like to meet\", 'all', 'DET'],\n",
       " [\"I was wondering if after all these years you'd like to meet\",\n",
       "  'these',\n",
       "  'DET'],\n",
       " [\"I was wondering if after all these years you'd like to meet\",\n",
       "  'year',\n",
       "  'NOUN'],\n",
       " [\"I was wondering if after all these years you'd like to meet\",\n",
       "  'you',\n",
       "  'PRON'],\n",
       " [\"I was wondering if after all these years you'd like to meet\",\n",
       "  'like',\n",
       "  'VERB'],\n",
       " [\"I was wondering if after all these years you'd like to meet\", 'to', 'PART'],\n",
       " [\"I was wondering if after all these years you'd like to meet\",\n",
       "  'meet',\n",
       "  'VERB'],\n",
       " ['To go over everything', 'to', 'PART'],\n",
       " ['To go over everything', 'go', 'VERB'],\n",
       " ['To go over everything', 'over', 'ADP'],\n",
       " ['To go over everything', 'everything', 'PRON'],\n",
       " [\"They say that time's supposed to heal ya, but I ain't done much healing\",\n",
       "  'PRON'],\n",
       " [\"They say that time's supposed to heal ya, but I ain't done much healing\",\n",
       "  'say',\n",
       "  'VERB'],\n",
       " [\"They say that time's supposed to heal ya, but I ain't done much healing\",\n",
       "  'that',\n",
       "  'DET'],\n",
       " [\"They say that time's supposed to heal ya, but I ain't done much healing\",\n",
       "  'time',\n",
       "  'NOUN'],\n",
       " [\"They say that time's supposed to heal ya, but I ain't done much healing\",\n",
       "  'suppose',\n",
       "  'VERB'],\n",
       " [\"They say that time's supposed to heal ya, but I ain't done much healing\",\n",
       "  'to',\n",
       "  'PART'],\n",
       " [\"They say that time's supposed to heal ya, but I ain't done much healing\",\n",
       "  'heal',\n",
       "  'VERB'],\n",
       " [\"They say that time's supposed to heal ya, but I ain't done much healing\",\n",
       "  'ya',\n",
       "  'PRON'],\n",
       " [\"They say that time's supposed to heal ya, but I ain't done much healing\",\n",
       "  'but',\n",
       "  'CCONJ'],\n",
       " [\"They say that time's supposed to heal ya, but I ain't done much healing\",\n",
       "  'I',\n",
       "  'PRON'],\n",
       " [\"They say that time's supposed to heal ya, but I ain't done much healing\",\n",
       "  'be',\n",
       "  'VERB'],\n",
       " [\"They say that time's supposed to heal ya, but I ain't done much healing\",\n",
       "  'not',\n",
       "  'PART'],\n",
       " [\"They say that time's supposed to heal ya, but I ain't done much healing\",\n",
       "  'do',\n",
       "  'VERB'],\n",
       " [\"They say that time's supposed to heal ya, but I ain't done much healing\",\n",
       "  'much',\n",
       "  'ADJ'],\n",
       " [\"They say that time's supposed to heal ya, but I ain't done much healing\",\n",
       "  'healing',\n",
       "  'NOUN'],\n",
       " ['Hello, can you hear me?', 'hello', 'INTJ'],\n",
       " ['Hello, can you hear me?', 'can', 'VERB'],\n",
       " ['Hello, can you hear me?', 'you', 'PRON'],\n",
       " ['Hello, can you hear me?', 'hear', 'VERB'],\n",
       " ['Hello, can you hear me?', 'I', 'PRON'],\n",
       " [\"I'm in California dreaming about who we used to be\", 'I', 'PRON'],\n",
       " [\"I'm in California dreaming about who we used to be\", 'be', 'AUX'],\n",
       " [\"I'm in California dreaming about who we used to be\", 'in', 'ADP'],\n",
       " [\"I'm in California dreaming about who we used to be\", 'California', 'PROPN'],\n",
       " [\"I'm in California dreaming about who we used to be\", 'dream', 'VERB'],\n",
       " [\"I'm in California dreaming about who we used to be\", 'about', 'ADP'],\n",
       " [\"I'm in California dreaming about who we used to be\", 'who', 'PRON'],\n",
       " [\"I'm in California dreaming about who we used to be\", 'PRON'],\n",
       " [\"I'm in California dreaming about who we used to be\", 'use', 'VERB'],\n",
       " [\"I'm in California dreaming about who we used to be\", 'to', 'PART'],\n",
       " [\"I'm in California dreaming about who we used to be\", 'be', 'AUX'],\n",
       " ['When we were younger and free', 'when', 'ADV'],\n",
       " ['When we were younger and free', 'PRON'],\n",
       " ['When we were younger and free', 'be', 'AUX'],\n",
       " ['When we were younger and free', 'young', 'ADJ'],\n",
       " ['When we were younger and free', 'and', 'CCONJ'],\n",
       " ['When we were younger and free', 'free', 'ADJ'],\n",
       " [\"I've forgotten how it felt before the world fell at our feet\", 'I', 'PRON'],\n",
       " [\"I've forgotten how it felt before the world fell at our feet\",\n",
       "  'have',\n",
       "  'AUX'],\n",
       " [\"I've forgotten how it felt before the world fell at our feet\",\n",
       "  'forget',\n",
       "  'VERB'],\n",
       " [\"I've forgotten how it felt before the world fell at our feet\",\n",
       "  'how',\n",
       "  'ADV'],\n",
       " [\"I've forgotten how it felt before the world fell at our feet\",\n",
       "  'it',\n",
       "  'PRON'],\n",
       " [\"I've forgotten how it felt before the world fell at our feet\",\n",
       "  'feel',\n",
       "  'VERB'],\n",
       " [\"I've forgotten how it felt before the world fell at our feet\",\n",
       "  'before',\n",
       "  'ADP'],\n",
       " [\"I've forgotten how it felt before the world fell at our feet\",\n",
       "  'the',\n",
       "  'DET'],\n",
       " [\"I've forgotten how it felt before the world fell at our feet\",\n",
       "  'world',\n",
       "  'NOUN'],\n",
       " [\"I've forgotten how it felt before the world fell at our feet\",\n",
       "  'fall',\n",
       "  'VERB'],\n",
       " [\"I've forgotten how it felt before the world fell at our feet\", 'at', 'ADP'],\n",
       " [\"I've forgotten how it felt before the world fell at our feet\", 'DET'],\n",
       " [\"I've forgotten how it felt before the world fell at our feet\",\n",
       "  'foot',\n",
       "  'NOUN'],\n",
       " [\"There's such a difference between us\", 'there', 'PRON'],\n",
       " [\"There's such a difference between us\", 'be', 'AUX'],\n",
       " [\"There's such a difference between us\", 'such', 'DET'],\n",
       " [\"There's such a difference between us\", 'a', 'DET'],\n",
       " [\"There's such a difference between us\", 'difference', 'NOUN'],\n",
       " [\"There's such a difference between us\", 'between', 'ADP'],\n",
       " [\"There's such a difference between us\", 'PRON'],\n",
       " ['And a million miles', 'and', 'CCONJ'],\n",
       " ['And a million miles', 'a', 'DET'],\n",
       " ['And a million miles', 'million', 'NUM'],\n",
       " ['And a million miles', 'mile', 'NOUN'],\n",
       " ['Hello from the other side', 'hello', 'INTJ'],\n",
       " ['Hello from the other side', 'from', 'ADP'],\n",
       " ['Hello from the other side', 'the', 'DET'],\n",
       " ['Hello from the other side', 'other', 'ADJ'],\n",
       " ['Hello from the other side', 'side', 'NOUN'],\n",
       " [\"I must've called a thousand times\", 'I', 'PRON'],\n",
       " [\"I must've called a thousand times\", 'must', 'VERB'],\n",
       " [\"I must've called a thousand times\", 'have', 'AUX'],\n",
       " [\"I must've called a thousand times\", 'call', 'VERB'],\n",
       " [\"I must've called a thousand times\", 'a', 'DET'],\n",
       " [\"I must've called a thousand times\", 'thousand', 'NUM'],\n",
       " [\"I must've called a thousand times\", 'time', 'NOUN'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'to', 'PART'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'tell', 'VERB'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'you', 'PRON'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'I', 'PRON'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'be', 'AUX'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'sorry', 'ADJ'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'for', 'ADP'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'everything', 'PRON'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'that', 'DET'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'I', 'PRON'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'have', 'AUX'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'do', 'VERB'],\n",
       " ['But when I call, you never seem to be home', 'but', 'CCONJ'],\n",
       " ['But when I call, you never seem to be home', 'when', 'ADV'],\n",
       " ['But when I call, you never seem to be home', 'I', 'PRON'],\n",
       " ['But when I call, you never seem to be home', 'call', 'VERB'],\n",
       " ['But when I call, you never seem to be home', 'you', 'PRON'],\n",
       " ['But when I call, you never seem to be home', 'never', 'ADV'],\n",
       " ['But when I call, you never seem to be home', 'seem', 'VERB'],\n",
       " ['But when I call, you never seem to be home', 'to', 'PART'],\n",
       " ['But when I call, you never seem to be home', 'be', 'AUX'],\n",
       " ['But when I call, you never seem to be home', 'home', 'ADV'],\n",
       " ['Hello from the outside', 'hello', 'INTJ'],\n",
       " ['Hello from the outside', 'from', 'ADP'],\n",
       " ['Hello from the outside', 'the', 'DET'],\n",
       " ['Hello from the outside', 'outside', 'NOUN'],\n",
       " [\"At least, I can say that I've tried\", 'at', 'ADV'],\n",
       " [\"At least, I can say that I've tried\", 'least', 'ADV'],\n",
       " [\"At least, I can say that I've tried\", 'I', 'PRON'],\n",
       " [\"At least, I can say that I've tried\", 'can', 'VERB'],\n",
       " [\"At least, I can say that I've tried\", 'say', 'VERB'],\n",
       " [\"At least, I can say that I've tried\", 'that', 'SCONJ'],\n",
       " [\"At least, I can say that I've tried\", 'I', 'PRON'],\n",
       " [\"At least, I can say that I've tried\", 'have', 'AUX'],\n",
       " [\"At least, I can say that I've tried\", 'try', 'VERB'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'to', 'PART'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'tell', 'VERB'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'you', 'PRON'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'I', 'PRON'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'be', 'AUX'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'sorry', 'ADJ'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'for', 'ADP'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'break', 'VERB'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'you', 'DET'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'heart', 'NOUN'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'but',\n",
       "  'CCONJ'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'it',\n",
       "  'PRON'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'do',\n",
       "  'AUX'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'not',\n",
       "  'PART'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'matter',\n",
       "  'VERB'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'it',\n",
       "  'PRON'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'clearly',\n",
       "  'ADV'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'do',\n",
       "  'AUX'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'not',\n",
       "  'PART'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'tear',\n",
       "  'VERB'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'you',\n",
       "  'PRON'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'apart',\n",
       "  'ADV'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'anymore',\n",
       "  'ADV'],\n",
       " ['Hello, how are you?', 'hello', 'INTJ'],\n",
       " ['Hello, how are you?', 'how', 'ADV'],\n",
       " ['Hello, how are you?', 'be', 'AUX'],\n",
       " ['Hello, how are you?', 'you', 'PRON'],\n",
       " [\"It's so typical of me to talk about myself, I'm sorry\", 'PRON'],\n",
       " [\"It's so typical of me to talk about myself, I'm sorry\", 'be', 'AUX'],\n",
       " [\"It's so typical of me to talk about myself, I'm sorry\", 'so', 'ADV'],\n",
       " [\"It's so typical of me to talk about myself, I'm sorry\", 'typical', 'ADJ'],\n",
       " [\"It's so typical of me to talk about myself, I'm sorry\", 'of', 'ADP'],\n",
       " [\"It's so typical of me to talk about myself, I'm sorry\", 'I', 'PRON'],\n",
       " [\"It's so typical of me to talk about myself, I'm sorry\", 'to', 'PART'],\n",
       " [\"It's so typical of me to talk about myself, I'm sorry\", 'talk', 'VERB'],\n",
       " [\"It's so typical of me to talk about myself, I'm sorry\", 'about', 'ADP'],\n",
       " [\"It's so typical of me to talk about myself, I'm sorry\", 'I', 'PRON'],\n",
       " [\"It's so typical of me to talk about myself, I'm sorry\", 'I', 'PRON'],\n",
       " [\"It's so typical of me to talk about myself, I'm sorry\", 'be', 'AUX'],\n",
       " [\"It's so typical of me to talk about myself, I'm sorry\", 'sorry', 'ADJ'],\n",
       " [\"I hope that you're well\", 'I', 'PRON'],\n",
       " [\"I hope that you're well\", 'hope', 'VERB'],\n",
       " [\"I hope that you're well\", 'that', 'SCONJ'],\n",
       " [\"I hope that you're well\", 'you', 'PRON'],\n",
       " [\"I hope that you're well\", 'be', 'AUX'],\n",
       " [\"I hope that you're well\", 'well', 'ADV'],\n",
       " ['Did you ever make it out of that town where nothing ever happened?',\n",
       "  'do',\n",
       "  'AUX'],\n",
       " ['Did you ever make it out of that town where nothing ever happened?',\n",
       "  'you',\n",
       "  'PRON'],\n",
       " ['Did you ever make it out of that town where nothing ever happened?',\n",
       "  'ever',\n",
       "  'ADV'],\n",
       " ['Did you ever make it out of that town where nothing ever happened?',\n",
       "  'make',\n",
       "  'VERB'],\n",
       " ['Did you ever make it out of that town where nothing ever happened?',\n",
       "  'it',\n",
       "  'PRON'],\n",
       " ['Did you ever make it out of that town where nothing ever happened?',\n",
       "  'out',\n",
       "  'SCONJ'],\n",
       " ['Did you ever make it out of that town where nothing ever happened?',\n",
       "  'of',\n",
       "  'ADP'],\n",
       " ['Did you ever make it out of that town where nothing ever happened?',\n",
       "  'that',\n",
       "  'DET'],\n",
       " ['Did you ever make it out of that town where nothing ever happened?',\n",
       "  'town',\n",
       "  'NOUN'],\n",
       " ['Did you ever make it out of that town where nothing ever happened?',\n",
       "  'where',\n",
       "  'ADV'],\n",
       " ['Did you ever make it out of that town where nothing ever happened?',\n",
       "  'nothing',\n",
       "  'PRON'],\n",
       " ['Did you ever make it out of that town where nothing ever happened?',\n",
       "  'ever',\n",
       "  'ADV'],\n",
       " ['Did you ever make it out of that town where nothing ever happened?',\n",
       "  'happen',\n",
       "  'VERB'],\n",
       " [\"It's no secret that the both of us\", 'PRON'],\n",
       " [\"It's no secret that the both of us\", 'be', 'AUX'],\n",
       " [\"It's no secret that the both of us\", 'no', 'DET'],\n",
       " [\"It's no secret that the both of us\", 'secret', 'NOUN'],\n",
       " [\"It's no secret that the both of us\", 'that', 'SCONJ'],\n",
       " [\"It's no secret that the both of us\", 'the', 'DET'],\n",
       " [\"It's no secret that the both of us\", 'both', 'DET'],\n",
       " [\"It's no secret that the both of us\", 'of', 'ADP'],\n",
       " [\"It's no secret that the both of us\", 'PRON'],\n",
       " ['Are running out of time', 'be', 'AUX'],\n",
       " ['Are running out of time', 'run', 'VERB'],\n",
       " ['Are running out of time', 'out', 'SCONJ'],\n",
       " ['Are running out of time', 'of', 'ADP'],\n",
       " ['Are running out of time', 'time', 'NOUN'],\n",
       " ['So hello from the other side', 'so', 'ADV'],\n",
       " ['So hello from the other side', 'hello', 'INTJ'],\n",
       " ['So hello from the other side', 'from', 'ADP'],\n",
       " ['So hello from the other side', 'the', 'DET'],\n",
       " ['So hello from the other side', 'other', 'ADJ'],\n",
       " ['So hello from the other side', 'side', 'NOUN'],\n",
       " [\"I must've called a thousand times\", 'I', 'PRON'],\n",
       " [\"I must've called a thousand times\", 'must', 'VERB'],\n",
       " [\"I must've called a thousand times\", 'have', 'AUX'],\n",
       " [\"I must've called a thousand times\", 'call', 'VERB'],\n",
       " [\"I must've called a thousand times\", 'a', 'DET'],\n",
       " [\"I must've called a thousand times\", 'thousand', 'NUM'],\n",
       " [\"I must've called a thousand times\", 'time', 'NOUN'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'to', 'PART'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'tell', 'VERB'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'you', 'PRON'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'I', 'PRON'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'be', 'AUX'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'sorry', 'ADJ'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'for', 'ADP'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'everything', 'PRON'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'that', 'DET'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'I', 'PRON'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'have', 'AUX'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'do', 'VERB'],\n",
       " ['But when I call, you never seem to be home', 'but', 'CCONJ'],\n",
       " ['But when I call, you never seem to be home', 'when', 'ADV'],\n",
       " ['But when I call, you never seem to be home', 'I', 'PRON'],\n",
       " ['But when I call, you never seem to be home', 'call', 'VERB'],\n",
       " ['But when I call, you never seem to be home', 'you', 'PRON'],\n",
       " ['But when I call, you never seem to be home', 'never', 'ADV'],\n",
       " ['But when I call, you never seem to be home', 'seem', 'VERB'],\n",
       " ['But when I call, you never seem to be home', 'to', 'PART'],\n",
       " ['But when I call, you never seem to be home', 'be', 'AUX'],\n",
       " ['But when I call, you never seem to be home', 'home', 'ADV'],\n",
       " ['Hello from the outside', 'hello', 'INTJ'],\n",
       " ['Hello from the outside', 'from', 'ADP'],\n",
       " ['Hello from the outside', 'the', 'DET'],\n",
       " ['Hello from the outside', 'outside', 'NOUN'],\n",
       " [\"At least, I can say that I've tried\", 'at', 'ADV'],\n",
       " [\"At least, I can say that I've tried\", 'least', 'ADV'],\n",
       " [\"At least, I can say that I've tried\", 'I', 'PRON'],\n",
       " [\"At least, I can say that I've tried\", 'can', 'VERB'],\n",
       " [\"At least, I can say that I've tried\", 'say', 'VERB'],\n",
       " [\"At least, I can say that I've tried\", 'that', 'SCONJ'],\n",
       " [\"At least, I can say that I've tried\", 'I', 'PRON'],\n",
       " [\"At least, I can say that I've tried\", 'have', 'AUX'],\n",
       " [\"At least, I can say that I've tried\", 'try', 'VERB'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'to', 'PART'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'tell', 'VERB'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'you', 'PRON'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'I', 'PRON'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'be', 'AUX'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'sorry', 'ADJ'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'for', 'ADP'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'break', 'VERB'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'you', 'DET'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'heart', 'NOUN'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'but',\n",
       "  'CCONJ'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'it',\n",
       "  'PRON'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'do',\n",
       "  'AUX'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'not',\n",
       "  'PART'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'matter',\n",
       "  'VERB'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'it',\n",
       "  'PRON'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'clearly',\n",
       "  'ADV'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'do',\n",
       "  'AUX'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'not',\n",
       "  'PART'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'tear',\n",
       "  'VERB'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'you',\n",
       "  'PRON'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'apart',\n",
       "  'ADV'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'anymore',\n",
       "  'ADV'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'high', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'high', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'high', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'high', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'low', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'low', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'low', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'low', 'NOUN'],\n",
       " ['Ooh, anymore', 'ooh', 'ADV'],\n",
       " ['Ooh, anymore', 'anymore', 'ADV'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'high', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'high', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'high', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'high', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'low', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'low', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'low', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'low', 'NOUN'],\n",
       " ['Ooh, anymore', 'ooh', 'ADV'],\n",
       " ['Ooh, anymore', 'anymore', 'ADV'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'high', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'high', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'high', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'high', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'low', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'low', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'low', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'low', 'NOUN'],\n",
       " ['Ooh, anymore', 'ooh', 'ADV'],\n",
       " ['Ooh, anymore', 'anymore', 'ADV'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'high', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'high', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'high', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'high', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'low', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'low', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'low', 'NOUN'],\n",
       " ['(Highs, highs, highs, highs, lows, lows, lows, lows)', 'low', 'NOUN'],\n",
       " ['Anymore', 'anymore', 'ADV'],\n",
       " ['Hello from the other side', 'hello', 'INTJ'],\n",
       " ['Hello from the other side', 'from', 'ADP'],\n",
       " ['Hello from the other side', 'the', 'DET'],\n",
       " ['Hello from the other side', 'other', 'ADJ'],\n",
       " ['Hello from the other side', 'side', 'NOUN'],\n",
       " [\"I must've called a thousand times\", 'I', 'PRON'],\n",
       " [\"I must've called a thousand times\", 'must', 'VERB'],\n",
       " [\"I must've called a thousand times\", 'have', 'AUX'],\n",
       " [\"I must've called a thousand times\", 'call', 'VERB'],\n",
       " [\"I must've called a thousand times\", 'a', 'DET'],\n",
       " [\"I must've called a thousand times\", 'thousand', 'NUM'],\n",
       " [\"I must've called a thousand times\", 'time', 'NOUN'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'to', 'PART'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'tell', 'VERB'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'you', 'PRON'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'I', 'PRON'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'be', 'AUX'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'sorry', 'ADJ'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'for', 'ADP'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'everything', 'PRON'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'that', 'DET'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'I', 'PRON'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'have', 'AUX'],\n",
       " [\"To tell you I'm sorry for everything that I've done\", 'do', 'VERB'],\n",
       " ['But when I call, you never seem to be home', 'but', 'CCONJ'],\n",
       " ['But when I call, you never seem to be home', 'when', 'ADV'],\n",
       " ['But when I call, you never seem to be home', 'I', 'PRON'],\n",
       " ['But when I call, you never seem to be home', 'call', 'VERB'],\n",
       " ['But when I call, you never seem to be home', 'you', 'PRON'],\n",
       " ['But when I call, you never seem to be home', 'never', 'ADV'],\n",
       " ['But when I call, you never seem to be home', 'seem', 'VERB'],\n",
       " ['But when I call, you never seem to be home', 'to', 'PART'],\n",
       " ['But when I call, you never seem to be home', 'be', 'AUX'],\n",
       " ['But when I call, you never seem to be home', 'home', 'ADV'],\n",
       " ['Hello from the outside', 'hello', 'INTJ'],\n",
       " ['Hello from the outside', 'from', 'ADP'],\n",
       " ['Hello from the outside', 'the', 'DET'],\n",
       " ['Hello from the outside', 'outside', 'NOUN'],\n",
       " [\"At least, I can say that I've tried\", 'at', 'ADV'],\n",
       " [\"At least, I can say that I've tried\", 'least', 'ADV'],\n",
       " [\"At least, I can say that I've tried\", 'I', 'PRON'],\n",
       " [\"At least, I can say that I've tried\", 'can', 'VERB'],\n",
       " [\"At least, I can say that I've tried\", 'say', 'VERB'],\n",
       " [\"At least, I can say that I've tried\", 'that', 'SCONJ'],\n",
       " [\"At least, I can say that I've tried\", 'I', 'PRON'],\n",
       " [\"At least, I can say that I've tried\", 'have', 'AUX'],\n",
       " [\"At least, I can say that I've tried\", 'try', 'VERB'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'to', 'PART'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'tell', 'VERB'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'you', 'PRON'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'I', 'PRON'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'be', 'AUX'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'sorry', 'ADJ'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'for', 'ADP'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'break', 'VERB'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'you', 'DET'],\n",
       " [\"To tell you I'm sorry for breaking your heart\", 'heart', 'NOUN'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'but',\n",
       "  'CCONJ'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'it',\n",
       "  'PRON'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'do',\n",
       "  'AUX'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'not',\n",
       "  'PART'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'matter',\n",
       "  'VERB'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'it',\n",
       "  'PRON'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'clearly',\n",
       "  'ADV'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'do',\n",
       "  'AUX'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'not',\n",
       "  'PART'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'tear',\n",
       "  'VERB'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'you',\n",
       "  'PRON'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'apart',\n",
       "  'ADV'],\n",
       " [\"But it don't matter, it clearly doesn't tear you apart anymore\",\n",
       "  'anymore',\n",
       "  'ADV']]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('hello.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_api():\n",
    "    \n",
    "    g_token = conf.G_TOKEN\n",
    "    api = genius.Genius(g_token)\n",
    "    genius.remove_section_headers = True\n",
    "    genius.excluded_terms = [\"(Remix)\", \"(Live)\", \"Remix\", \"mix\", \"Edition\", \n",
    "                             '(TraducciÃ³n al EspaÃ±ol)', '(TraduÃ§Ã£o em PortuguÃªs)', \n",
    "                             '(Deutsche Ãœbersetzung)', '(Turkish Translation)', \n",
    "                             '(Traduction FranÃ§aise)', '(TÃ¼rkÃ§e Ã‡eviri)', \n",
    "                             '(Traduzione Italiana)', '(ÄŒeskÃ½ PÅ™eklad)', \n",
    "                             '(Dansk OversÃ¦ttelse)', '(Traducerea RomÃ¢neascÄƒ)',\n",
    "                             '([0-9a-zA-Z -]* Remix)', '(Demo)']\n",
    "    genius.skip_non_songs = True\n",
    "    with open('artists and albums.txt', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    li = re.findall('([a-zA-Z0-9\\- ]*)\\n', text)\n",
    "    return (api, li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_database(api, li):\n",
    "    \n",
    "    os.makedirs(\"Database\", exist_ok=True)\n",
    "    for artist in li:\n",
    "        os.makedirs('corpus', exist_ok=True)\n",
    "        artist = artist.split()\n",
    "        name_dir = artist[0]\n",
    "        os.makedirs(name_dir, exist_ok=True)\n",
    "        for album in range(1, len(artist)):\n",
    "            album = artist[album]\n",
    "            link = 'https://genius.com/albums/' + artist[0] + '/' + album\n",
    "            result = requests.get(link)\n",
    "            html = result.text\n",
    "            soup = BeautifulSoup(html,'html.parser')\n",
    "            for song in soup.find_all('h3', {'class': 'chart_row-content-title'}):\n",
    "                song_name = song.get_text()\n",
    "                song_name = unicodedata.normalize(\"NFKD\", song_name)\n",
    "                song_name = re.search('( +)(.+)(\\n)', song_name).group(2)\n",
    "                try:\n",
    "                    song = api.search_song(song_name, artist[0])\n",
    "                    song_name = sanitize_filename(song_name).replace('-', '')\n",
    "                    album = album.replace('-', '')\n",
    "                    name = album + '_' + song_name\n",
    "                    for i in terms:\n",
    "                        if re.search(i, str(song)):\n",
    "                            song = None\n",
    "                    if song is not None: \n",
    "                        song.save_lyrics(extension='txt', filename= name, overwrite= True, binary_encoding=True)\n",
    "                        name = name + '.txt'\n",
    "                        shutil.move(name, 'corpus')\n",
    "                except (TypeError, RuntimeError) as ex:\n",
    "                    pass\n",
    "        shutil.move('corpus', name_dir)\n",
    "        shutil.move(name_dir, 'Database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'artists and albums.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-ec8cf33848db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mli\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_api\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprepare_database\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mli\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-70-c0b2ff311154>\u001b[0m in \u001b[0;36mmake_api\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m                              '([0-9a-zA-Z -]* Remix)', '(Demo)']\n\u001b[0;32m     13\u001b[0m     \u001b[0mgenius\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mskip_non_songs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'artists and albums.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mli\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'([a-zA-Z0-9\\- ]*)\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'artists and albums.txt'"
     ]
    }
   ],
   "source": [
    "api, li = make_api()\n",
    "prepare_database(api, li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_links(li):\n",
    "    for artist in li:\n",
    "        artist = artist.split()\n",
    "        for album in range(1, len(artist)):\n",
    "            album = artist[album]\n",
    "            link = 'https://genius.com/albums/' + artist[0] + '/' + album\n",
    "            result = requests.get(link)\n",
    "            html = result.text\n",
    "            if re.search('Oops', html):\n",
    "                print(album, '!!!!!!!!!!!!!!!!NOT OK!!!!!!!!!!!!!!!!')\n",
    "            else:\n",
    "                print(album,'OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(artists):\n",
    "    strings = []\n",
    "    for artist in artists:\n",
    "        albums = []\n",
    "        albums.append(artist.lower())\n",
    "        artist = sp.search(q='artist:' + artist, type='artist')\n",
    "        artist = artist['artists']['items'][0]['id']\n",
    "        alb = sp.artist_albums(artist)\n",
    "        for i in range(len(alb['items'])):\n",
    "            a = alb['items'][i]['name'].lower()\n",
    "            b = re.search('(.*)(?: \\(.*| -.*|:.*)(?:instrumentals|edition|complete|reissue|remastered|japan|anniversary|special|deluxe)', a)\n",
    "            c = re.search('(?:(.*)(?: \\(.*| -.*|:.*)(?:live|acoustic|soundtrack|remixes|mixes)|mtv|itunes|live (?:from|at|in)|bbc|radio|stadium|karaoke|[a-z]-sides|audio|version|session)', a)\n",
    "            if b != None:\n",
    "                a = b.group(1)\n",
    "            if alb['items'][i]['album_group'] == 'album' and a not in albums[1:] and c == None:\n",
    "                albums.append(a)\n",
    "        strings.append(albums)\n",
    "    return strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_for_genius(name):\n",
    "    name = re.sub('[^A-Za-z0-9]+', '-', name)\n",
    "    if name.endswith('-'):\n",
    "        name = name[:-1]\n",
    "    if name.startswith('-'):\n",
    "        name = name[1:]\n",
    "        \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lp_list(strings):\n",
    "\n",
    "    for i in range(len(strings)):\n",
    "        for j in range(len(strings[i])):\n",
    "            strings[i][j] = link_for_genius(strings[i][j])\n",
    "    st = []\n",
    "    for i in strings:\n",
    "        s = ' '.join(i)\n",
    "        st.append(s)\n",
    "    st = '\\n'.join(st)\n",
    "    \n",
    "    with open('artists and albums.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open('hip_hop.txt', encoding='utf-8') as f:\n",
    "    artists = f.read()\n",
    "    artists = artists.splitlines()\n",
    "    \n",
    "\n",
    "t = prepare(artists)\n",
    "with open('hip-hop_albums.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(str(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ed Sheeran',\n",
       " 'Justin Timberlake',\n",
       " 'Adele',\n",
       " 'One Direction',\n",
       " 'Taylor Swift',\n",
       " 'Katy Perry',\n",
       " 'Lady Gaga',\n",
       " 'Bruno Mars ',\n",
       " 'The Weekend',\n",
       " 'Sia']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ed sheeran', 'no.6 collaborations project', 'Ã·', 'x', '5', 'live at the bedford', 'loose change', '+', 'no.5 collaborations project']\n",
      "['justin timberlake', 'man of the woods', 'the 20/20 experience - 2 of 2', 'the 20/20 experience', 'futuresex/lovesounds deluxe edition', 'futuresex/lovesounds', 'justified']\n",
      "['adele', '25', '21', '19']\n",
      "['one direction', 'made in the a.m.', 'four', 'midnight memories', 'take me home', 'up all night']\n",
      "['taylor swift', 'folklore', 'lover', 'taylor swift karaoke: reputation', 'reputation', 'reputation stadium tour surprise song playlist', 'taylor swift karaoke: 1989', '1989']\n",
      "['katy perry', 'smile', 'witness', 'prism', 'katy perry - teenage dream', 'teenage dream']\n",
      "['lady gaga', 'chromatica', 'a star is born soundtrack', 'a star is born soundtrack (without dialogue)', 'joanne', 'cheek to cheek', 'artpop', 'born this way']\n",
      "['bruno mars ', '24k magic', 'unorthodox jukebox', 'doo-wops & hooligans']\n",
      "['the weekend', 'after hours', 'my dear melancholy,', 'starboy', 'beauty behind the madness', 'kiss land', 'trilogy']\n",
      "['sia', 'labrinth, sia & diplo present... lsd', 'everyday is christmas', 'this is acting', '1000 forms of fear', 'we are born']\n"
     ]
    }
   ],
   "source": [
    "for artist in artists:\n",
    "    albums = []\n",
    "    albums.append(artist.lower())\n",
    "    artist = sp.search(q='artist:' + artist, type='artist')\n",
    "    artist = artist['artists']['items'][0]['id']\n",
    "    alb = sp.artist_albums(artist)\n",
    "    for i in range(len(alb['items'])):\n",
    "        a = alb['items'][i]['name'].lower()\n",
    "        b = re.search('(.*)(?: \\(.*| -.*|:.*)(?: instrumentals|edition|complete|reissue|remastered|japan|anniversary|special|deluxe)', a)\n",
    "        c = re.search('(.*)(?: \\(.*| -.*|:.*)(?: live|soundtrack|remixes|mixes)', a)\n",
    "        if b != None:\n",
    "            a = b.group(1)\n",
    "        if alb['items'][i]['album_group'] == 'album' and a not in albums[1:] and c == None:\n",
    "            albums.append(a)\n",
    "    print(albums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Bridge.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Anymore..Chorus., Bridge.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Anymore..Chorus., Bridge.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Anymore..Chorus., Bridge.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Anymore..Chorus., Bridge.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Anymore..Chorus., Bridge.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Anymore..Chorus., Bridge.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Anymore..Chorus., Bridge.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Anymore..Chorus., Bridge.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Anymore..Chorus., Bridge.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Anymore..Chorus., Bridge.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Anymore..Chorus., Bridge.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Ooh, anymore.(Highs, highs, highs, highs, lows, lows, lows, lows).Anymore..Chorus.]\n"
     ]
    }
   ],
   "source": [
    "def search(keywords):\n",
    "    res = []\n",
    "    keywords = keywords.split(\" \")\n",
    "    sentences = re.sub(\"\\n\", \".\", text)\n",
    "    sentences = re.sub(\"\\[\", \"\", sentences)\n",
    "    sentences = re.sub(\"]\", \"\", sentences)\n",
    "    doc = nlp(sentences)\n",
    "    for sentence in doc.sents:\n",
    "        for i, token in enumerate(sentence):\n",
    "            if len(sentence) - i > len(keywords):\n",
    "                if match(sentence[i:], keywords):\n",
    "                    res.append(sentence)\n",
    "    return res\n",
    "\n",
    "\n",
    "def match(tokens, keywords):\n",
    "    testlist = tokens[:len(keywords)]\n",
    "    for i in range(len(keywords)):\n",
    "        keywords[i] = str(keywords[i])\n",
    "        if testlist[i].pos_ == keywords[i]:\n",
    "            continue\n",
    "        elif keywords[i].startswith(\"\\\"\") and keywords[i].endswith(\"\\\"\"):\n",
    "            if keywords[i][1:-1] != testlist[i].text:\n",
    "                return False\n",
    "        elif \"+\" in keywords[i]:\n",
    "            word, pos = keywords[i].split(\"+\")\n",
    "            if testlist[i].pos_ != pos or testlist[i].text != word:\n",
    "                return False\n",
    "            else:\n",
    "                continue\n",
    "        elif testlist[i].lemma_ != next(nlp(keywords[i]).sents)[0].lemma_:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "#print(search('\"breaking\" DET heart+NOUN'))\n",
    "print(search('higher'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "[Verse 1]\n",
    "Hello, it's me\n",
    "I was wondering if after all these years you'd like to meet\n",
    "To go over everything\n",
    "They say that time's supposed to heal ya, but I ain't done much healing\n",
    "Hello, can you hear me?\n",
    "I'm in California dreaming about who we used to be\n",
    "When we were younger and free\n",
    "I've forgotten how it felt before the world fell at our feet\n",
    "\n",
    "[Pre-Chorus]\n",
    "There's such a difference between us\n",
    "And a million miles\n",
    "\n",
    "[Chorus]\n",
    "Hello from the other side\n",
    "I must've called a thousand times\n",
    "To tell you I'm sorry for everything that I've done\n",
    "But when I call, you never seem to be home\n",
    "Hello from the outside\n",
    "At least, I can say that I've tried\n",
    "To tell you I'm sorry for breaking your heart\n",
    "But it don't matter, it clearly doesn't tear you apart anymore\n",
    "\n",
    "[Verse 2]\n",
    "Hello, how are you?\n",
    "It's so typical of me to talk about myself, I'm sorry\n",
    "I hope that you're well\n",
    "Did you ever make it out of that town where nothing ever happened?\n",
    "\n",
    "[Pre-Chorus]\n",
    "It's no secret that the both of us\n",
    "Are running out of time\n",
    "\n",
    "[Chorus]\n",
    "So hello from the other side\n",
    "I must've called a thousand times\n",
    "To tell you I'm sorry for everything that I've done\n",
    "But when I call, you never seem to be home\n",
    "Hello from the outside\n",
    "At least, I can say that I've tried\n",
    "To tell you I'm sorry for breaking your heart\n",
    "But it don't matter, it clearly doesn't tear you apart anymore\n",
    "\n",
    "[Bridge]\n",
    "(Highs, highs, highs, highs, lows, lows, lows, lows)\n",
    "Ooh, anymore\n",
    "(Highs, highs, highs, highs, lows, lows, lows, lows)\n",
    "Ooh, anymore\n",
    "(Highs, highs, highs, highs, lows, lows, lows, lows)\n",
    "Ooh, anymore\n",
    "(Highs, highs, highs, highs, lows, lows, lows, lows)\n",
    "Anymore\n",
    "\n",
    "[Chorus]\n",
    "Hello from the other side\n",
    "I must've called a thousand times\n",
    "To tell you I'm sorry for everything that I've done\n",
    "But when I call, you never seem to be home\n",
    "Hello from the outside\n",
    "At least, I can say that I've tried\n",
    "To tell you I'm sorry for breaking your heart\n",
    "But it don't matter, it clearly doesn't tear you apart anymore\n",
    "\n",
    "[Produced by Greg Kurstin]\n",
    "[Music Video]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
